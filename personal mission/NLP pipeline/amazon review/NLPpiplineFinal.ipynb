{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "655e1a0a-e131-494b-8cda-753a6dc8e655",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/eunma/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/eunma/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading data...\n",
      "[INFO] Data loaded successfully.\n",
      "[INFO] Exploring data...\n",
      "   Id   ProductId          UserId                      ProfileName  \\\n",
      "0   1  B001E4KFG0  A3SGXH7AUHU8GW                       delmartian   \n",
      "1   2  B00813GRG4  A1D87F6ZCVE5NK                           dll pa   \n",
      "2   3  B000LQOCH0   ABXLMWJIXXAIN  Natalia Corres \"Natalia Corres\"   \n",
      "3   4  B000UA0QIQ  A395BORC6FGVXV                             Karl   \n",
      "4   5  B006K2ZZ7K  A1UQRSCLF8GW1T    Michael D. Bigham \"M. Wassir\"   \n",
      "\n",
      "   HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
      "0                     1                       1      5  1303862400   \n",
      "1                     0                       0      1  1346976000   \n",
      "2                     1                       1      4  1219017600   \n",
      "3                     3                       3      2  1307923200   \n",
      "4                     0                       0      5  1350777600   \n",
      "\n",
      "                 Summary                                               Text  \n",
      "0  Good Quality Dog Food  I have bought several of the Vitality canned d...  \n",
      "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...  \n",
      "2  \"Delight\" says it all  This is a confection that has been around a fe...  \n",
      "3         Cough Medicine  If you are looking for the secret ingredient i...  \n",
      "4            Great taffy  Great taffy at a great price.  There was a wid...  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 568454 entries, 0 to 568453\n",
      "Data columns (total 10 columns):\n",
      " #   Column                  Non-Null Count   Dtype \n",
      "---  ------                  --------------   ----- \n",
      " 0   Id                      568454 non-null  int64 \n",
      " 1   ProductId               568454 non-null  object\n",
      " 2   UserId                  568454 non-null  object\n",
      " 3   ProfileName             568428 non-null  object\n",
      " 4   HelpfulnessNumerator    568454 non-null  int64 \n",
      " 5   HelpfulnessDenominator  568454 non-null  int64 \n",
      " 6   Score                   568454 non-null  int64 \n",
      " 7   Time                    568454 non-null  int64 \n",
      " 8   Summary                 568427 non-null  object\n",
      " 9   Text                    568454 non-null  object\n",
      "dtypes: int64(5), object(5)\n",
      "memory usage: 43.4+ MB\n",
      "None\n",
      "[INFO] Selected necessary columns.\n",
      "[INFO] Removed missing values.\n",
      "[INFO] Preprocessing text...\n",
      "                                                Text  Score  \\\n",
      "0  I have bought several of the Vitality canned d...      5   \n",
      "1  Product arrived labeled as Jumbo Salted Peanut...      1   \n",
      "2  This is a confection that has been around a fe...      4   \n",
      "3  If you are looking for the secret ingredient i...      2   \n",
      "4  Great taffy at a great price.  There was a wid...      5   \n",
      "\n",
      "                                      processed_text  \n",
      "0  bought several vitality canned dog food produc...  \n",
      "1  product arrived labeled jumbo salted peanutsth...  \n",
      "2  confection around centuries light pillowy citr...  \n",
      "3  looking secret ingredient robitussin believe f...  \n",
      "4  great taffy great price wide assortment yummy ...  \n",
      "[INFO] Text preprocessing completed.\n",
      "[INFO] Vectorizing text using TF-IDF...\n",
      "[INFO] Text vectorization completed.\n",
      "[INFO] Splitting data into train and test sets...\n",
      "[INFO] Data splitting completed.\n",
      "[INFO] Starting grid search for hyperparameter tuning...\n",
      "[INFO] Grid search completed.\n",
      "Best Hyperparameters: {'C': 1, 'penalty': 'l1', 'solver': 'liblinear'}\n",
      "[INFO] Evaluating model performance...\n",
      "Accuracy: 0.8920055237441838\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.67      0.73     24666\n",
      "           1       0.91      0.95      0.93     89025\n",
      "\n",
      "    accuracy                           0.89    113691\n",
      "   macro avg       0.86      0.81      0.83    113691\n",
      "weighted avg       0.89      0.89      0.89    113691\n",
      "\n",
      "Confusion Matrix:\n",
      " [[16578  8088]\n",
      " [ 4190 84835]]\n",
      "[INFO] Trained model saved as best_logistic_regression_model.pkl.\n",
      "[INFO] Processed data saved as processed_data.json.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import nltk\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# 작업 진행 상황 보고 함수\n",
    "def report_progress(message):\n",
    "    print(f\"[INFO] {message}\")\n",
    "\n",
    "# NLTK 불용어 다운로드\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 데이터 파일 경로\n",
    "file_path = '/Users/eunma/Documents/GitHub/kakaobootcamp/personal mission/NLP pipeline/data/Reviews.csv'\n",
    "\n",
    "# 데이터 로드\n",
    "report_progress(\"Loading data...\")\n",
    "data = pd.read_csv(file_path)\n",
    "report_progress(\"Data loaded successfully.\")\n",
    "\n",
    "# 데이터 탐색\n",
    "report_progress(\"Exploring data...\")\n",
    "print(data.head())\n",
    "print(data.info())\n",
    "\n",
    "# 데이터 선택 (필요한 열만 사용)\n",
    "data = data[['Text', 'Score']]\n",
    "report_progress(\"Selected necessary columns.\")\n",
    "\n",
    "# 결측값 제거\n",
    "data.dropna(inplace=True)\n",
    "report_progress(\"Removed missing values.\")\n",
    "\n",
    "# 리뷰 텍스트 전처리 함수\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # 소문자 변환\n",
    "    text = re.sub(r'\\d+', '', text)  # 숫자 제거\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # 구두점 제거\n",
    "    tokens = word_tokenize(text)  # 토큰화\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]  # 불용어 제거\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# 텍스트 전처리 적용\n",
    "report_progress(\"Preprocessing text...\")\n",
    "data['processed_text'] = data['Text'].apply(preprocess_text)\n",
    "print(data.head())\n",
    "report_progress(\"Text preprocessing completed.\")\n",
    "\n",
    "# TF-IDF 벡터화\n",
    "report_progress(\"Vectorizing text using TF-IDF...\")\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(data['processed_text'])\n",
    "y = data['Score']\n",
    "report_progress(\"Text vectorization completed.\")\n",
    "\n",
    "# 레이블 이진화 (positive: 4, 5, negative: 1, 2, 3)\n",
    "y = y.apply(lambda x: 1 if x > 3 else 0)\n",
    "\n",
    "# 데이터 분할\n",
    "report_progress(\"Splitting data into train and test sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "report_progress(\"Data splitting completed.\")\n",
    "\n",
    "# 하이퍼파라미터 그리드 설정\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100],  # 정규화 강도\n",
    "    'penalty': ['l1', 'l2'],       # 페널티 유형\n",
    "    'solver': ['liblinear']        # 최적화 알고리즘\n",
    "}\n",
    "\n",
    "# 로지스틱 회귀 모델 초기화\n",
    "log_reg = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# 그리드 서치 설정\n",
    "report_progress(\"Starting grid search for hyperparameter tuning...\")\n",
    "grid_search = GridSearchCV(estimator=log_reg, param_grid=param_grid, cv=5, n_jobs=-1, scoring='accuracy')\n",
    "\n",
    "# 그리드 서치 학습\n",
    "grid_search.fit(X_train, y_train)\n",
    "report_progress(\"Grid search completed.\")\n",
    "\n",
    "# 최적의 하이퍼파라미터 출력\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# 최적 모델로 예측\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# 성능 평가\n",
    "report_progress(\"Evaluating model performance...\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# 학습된 모델 저장\n",
    "model_filename = 'best_logistic_regression_model.pkl'\n",
    "with open(model_filename, 'wb') as file:\n",
    "    pickle.dump(best_model, file)\n",
    "report_progress(f\"Trained model saved as {model_filename}.\")\n",
    "\n",
    "# 데이터 저장\n",
    "data_filename = 'processed_data.json'\n",
    "data.to_json(data_filename, orient='records', lines=True)\n",
    "report_progress(f\"Processed data saved as {data_filename}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb5e7c1-c2a2-455d-b4c8-44da090b600d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
