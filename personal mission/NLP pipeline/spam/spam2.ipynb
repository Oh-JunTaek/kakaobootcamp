{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "707d5587-9880-441d-a001-1cdbdd978499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               label  text\n",
      "0  ham\\tGo until jurong point, crazy.. Available ...   NaN\n",
      "1                 ham\\tOk lar... Joking wif u oni...   NaN\n",
      "2  spam\\tFree entry in 2 a wkly comp to win FA Cu...   NaN\n",
      "3  ham\\tU dun say so early hor... U c already the...   NaN\n",
      "4  ham\\tNah I don't think he goes to usf, he live...   NaN\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5574 entries, 0 to 5573\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   label   5574 non-null   object \n",
      " 1   text    0 non-null      float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 87.2+ KB\n",
      "None\n",
      "                                               label  text  preprocessed_text\n",
      "0  ham\\tGo until jurong point, crazy.. Available ...   NaN                NaN\n",
      "1                 ham\\tOk lar... Joking wif u oni...   NaN                NaN\n",
      "2  spam\\tFree entry in 2 a wkly comp to win FA Cu...   NaN                NaN\n",
      "3  ham\\tU dun say so early hor... U c already the...   NaN                NaN\n",
      "4  ham\\tNah I don't think he goes to usf, he live...   NaN                NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# 데이터 로드\n",
    "data_path = 'C:\\\\Users\\\\dev\\\\Documents\\\\GitHub\\\\kakaobootcamp\\\\personal mission\\\\NLP pipeline\\\\data\\\\SMSSpamCollection.csv'\n",
    "df = pd.read_csv(data_path, sep='\\t', names=['label', 'text'])\n",
    "\n",
    "# 데이터 탐색\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "\n",
    "# 텍스트 전처리\n",
    "0def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # 소문자 변환\n",
    "        text = text.lower()\n",
    "        \n",
    "        # 특수문자 제거\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "        \n",
    "        # 토큰화\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        \n",
    "        # 불용어 제거\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        \n",
    "        # 어간 추출\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "        \n",
    "        # 토큰을 다시 텍스트로 결합\n",
    "        preprocessed_text = ' '.join(tokens)\n",
    "        \n",
    "        return preprocessed_text\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "# 데이터프레임에 전처리 적용\n",
    "df['preprocessed_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# 전처리된 데이터 확인\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0793888-acd0-4a8e-91f0-6f24b6b871f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               label  text\n",
      "0  ham\\tGo until jurong point, crazy.. Available ...   NaN\n",
      "1                 ham\\tOk lar... Joking wif u oni...   NaN\n",
      "2  spam\\tFree entry in 2 a wkly comp to win FA Cu...   NaN\n",
      "3  ham\\tU dun say so early hor... U c already the...   NaN\n",
      "4  ham\\tNah I don't think he goes to usf, he live...   NaN\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5574 entries, 0 to 5573\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   label   5574 non-null   object \n",
      " 1   text    0 non-null      float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 87.2+ KB\n",
      "None\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\dev/nltk_data'\n    - 'C:\\\\Users\\\\dev\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\dev\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\dev\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\dev\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:84\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 84\u001b[0m     root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords.zip/stopwords/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\dev/nltk_data'\n    - 'C:\\\\Users\\\\dev\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\dev\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\dev\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\dev\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 44\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# 데이터프레임에 전처리 적용\u001b[39;00m\n\u001b[0;32m     43\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 44\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessed_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(preprocess_text)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# 전처리된 데이터 확인\u001b[39;00m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\series.py:4764\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4629\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4630\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4631\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4636\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4637\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4638\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4639\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4640\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4755\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4756\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SeriesApply(\n\u001b[0;32m   4758\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4759\u001b[0m         func,\n\u001b[0;32m   4760\u001b[0m         convert_dtype\u001b[38;5;241m=\u001b[39mconvert_dtype,\n\u001b[0;32m   4761\u001b[0m         by_row\u001b[38;5;241m=\u001b[39mby_row,\n\u001b[0;32m   4762\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   4763\u001b[0m         kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[1;32m-> 4764\u001b[0m     )\u001b[38;5;241m.\u001b[39mapply()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1209\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_standard()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1289\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1287\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1288\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1289\u001b[0m mapped \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_map_values(\n\u001b[0;32m   1290\u001b[0m     mapper\u001b[38;5;241m=\u001b[39mcurried, na_action\u001b[38;5;241m=\u001b[39maction, convert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_dtype\n\u001b[0;32m   1291\u001b[0m )\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1295\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m algorithms\u001b[38;5;241m.\u001b[39mmap_array(arr, mapper, na_action\u001b[38;5;241m=\u001b[39mna_action, convert\u001b[38;5;241m=\u001b[39mconvert)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1812\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer(values, mapper, convert\u001b[38;5;241m=\u001b[39mconvert)\n\u001b[0;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1817\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1818\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2926\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[27], line 28\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     25\u001b[0m tokens \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(text)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# 불용어 제거\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m stop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     29\u001b[0m tokens \u001b[38;5;241m=\u001b[39m [token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stop_words]\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# 어간 추출\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:121\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m attr \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLazyCorpusLoader object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__bases__\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__load()\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# This looks circular, but its not, since __load() changes our\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# __class__ to something new:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:86\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m             root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mzip_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     85\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m:\n\u001b[1;32m---> 86\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Load the corpus.\u001b[39;00m\n\u001b[0;32m     89\u001b[0m corpus \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__reader_cls(root, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__kwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\corpus\\util.py:81\u001b[0m, in \u001b[0;36mLazyCorpusLoader.__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     80\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 81\u001b[0m         root \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubdir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mstopwords\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('stopwords')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/stopwords\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\dev/nltk_data'\n    - 'C:\\\\Users\\\\dev\\\\anaconda3\\\\nltk_data'\n    - 'C:\\\\Users\\\\dev\\\\anaconda3\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\dev\\\\anaconda3\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\dev\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# 데이터 로드\n",
    "data_path = 'C:\\\\Users\\\\dev\\\\Documents\\\\GitHub\\\\kakaobootcamp\\\\personal mission\\\\NLP pipeline\\\\data\\\\SMSSpamCollection.csv'\n",
    "df = pd.read_csv(data_path, sep='\\t', names=['label', 'text'])\n",
    "\n",
    "# 데이터 탐색\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "\n",
    "# 텍스트 전처리\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # 소문자 변환\n",
    "        text = text.lower()\n",
    "        \n",
    "        # 특수문자 제거\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "        \n",
    "        # 토큰화\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        \n",
    "        # 불용어 제거\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        \n",
    "        # 어간 추출\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "        \n",
    "        # 토큰을 다시 텍스트로 결합\n",
    "        preprocessed_text = ' '.join(tokens)\n",
    "        \n",
    "        return preprocessed_text\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# 데이터프레임에 전처리 적용\n",
    "df['text'] = df['text'].fillna('')\n",
    "df['preprocessed_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# 전처리된 데이터 확인\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f157d0a0-d3ea-4306-8031-c1dea76e4220",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dev\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b775368-0584-4bed-8157-bbea90c8bf22",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Expected 21 fields in line 3, saw 29. Error could possibly be due to quotes being ignored when a multi-char delimiter is used.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 데이터 로드\u001b[39;00m\n\u001b[0;32m      8\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdev\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mGitHub\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mkakaobootcamp\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mpersonal mission\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mNLP pipeline\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mSMSSpamCollection.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 9\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(data_path,  sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+\u001b[39m\u001b[38;5;124m'\u001b[39m, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m'\u001b[39m, names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# 데이터 탐색\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    945\u001b[0m )\n\u001b[0;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:617\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1748\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1741\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1743\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m     (\n\u001b[0;32m   1745\u001b[0m         index,\n\u001b[0;32m   1746\u001b[0m         columns,\n\u001b[0;32m   1747\u001b[0m         col_dict,\n\u001b[1;32m-> 1748\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1749\u001b[0m         nrows\n\u001b[0;32m   1750\u001b[0m     )\n\u001b[0;32m   1751\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1752\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:286\u001b[0m, in \u001b[0;36mPythonParser.read\u001b[1;34m(self, rows)\u001b[0m\n\u001b[0;32m    283\u001b[0m     indexnamerow \u001b[38;5;241m=\u001b[39m content[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    284\u001b[0m     content \u001b[38;5;241m=\u001b[39m content[\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m--> 286\u001b[0m alldata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rows_to_cols(content)\n\u001b[0;32m    287\u001b[0m data, columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exclude_implicit_index(alldata)\n\u001b[0;32m    289\u001b[0m conv_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_data(data)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:1058\u001b[0m, in \u001b[0;36mPythonParser._rows_to_cols\u001b[1;34m(self, content)\u001b[0m\n\u001b[0;32m   1052\u001b[0m             reason \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1053\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError could possibly be due to quotes being \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1054\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignored when a multi-char delimiter is used.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1055\u001b[0m             )\n\u001b[0;32m   1056\u001b[0m             msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m reason\n\u001b[1;32m-> 1058\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_alert_malformed(msg, row_num \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;66;03m# see gh-13320\u001b[39;00m\n\u001b[0;32m   1061\u001b[0m zipped_content \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(lib\u001b[38;5;241m.\u001b[39mto_object_array(content, min_width\u001b[38;5;241m=\u001b[39mcol_len)\u001b[38;5;241m.\u001b[39mT)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\python_parser.py:779\u001b[0m, in \u001b[0;36mPythonParser._alert_malformed\u001b[1;34m(self, msg, row_num)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    763\u001b[0m \u001b[38;5;124;03mAlert a user about a malformed row, depending on value of\u001b[39;00m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;124;03m`self.on_bad_lines` enum.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;124;03m    even though we 0-index internally.\u001b[39;00m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_bad_lines \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBadLineHandleMethod\u001b[38;5;241m.\u001b[39mERROR:\n\u001b[1;32m--> 779\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ParserError(msg)\n\u001b[0;32m    780\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_bad_lines \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mBadLineHandleMethod\u001b[38;5;241m.\u001b[39mWARN:\n\u001b[0;32m    781\u001b[0m     base \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipping line \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrow_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mParserError\u001b[0m: Expected 21 fields in line 3, saw 29. Error could possibly be due to quotes being ignored when a multi-char delimiter is used."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# 데이터 로드\n",
    "data_path = 'C:\\\\Users\\\\dev\\\\Documents\\\\GitHub\\\\kakaobootcamp\\\\personal mission\\\\NLP pipeline\\\\data\\\\SMSSpamCollection.csv'\n",
    "df = pd.read_csv(data_path,  sep='\\s+', engine='python', names=['label', 'text'])\n",
    "\n",
    "# 데이터 탐색\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "\n",
    "# 텍스트 전처리\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # 소문자 변환\n",
    "        text = text.lower()\n",
    "        \n",
    "        # 특수문자 제거\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "        \n",
    "        # 토큰화\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        \n",
    "        # 불용어 제거\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        \n",
    "        # 어간 추출\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "        \n",
    "        # 토큰을 다시 텍스트로 결합\n",
    "        preprocessed_text = ' '.join(tokens)\n",
    "        \n",
    "        return preprocessed_text\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# 데이터프레임에 전처리 적용\n",
    "df['text'] = df['text'].fillna('')\n",
    "df['preprocessed_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# 전처리된 데이터 확인\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "26f0f8c8-58b7-4cda-8ccc-f8c873dcf75b",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "read_csv() got an unexpected keyword argument 'error_bad_lines'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 데이터 로드\u001b[39;00m\n\u001b[0;32m      8\u001b[0m data_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdev\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDocuments\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mGitHub\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mkakaobootcamp\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mpersonal mission\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mNLP pipeline\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mSMSSpamCollection.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 9\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(data_path, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms+\u001b[39m\u001b[38;5;124m'\u001b[39m, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m'\u001b[39m, names\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m], error_bad_lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, warn_bad_lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# 데이터 탐색\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead())\n",
      "\u001b[1;31mTypeError\u001b[0m: read_csv() got an unexpected keyword argument 'error_bad_lines'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# 데이터 로드\n",
    "data_path = 'C:\\\\Users\\\\dev\\\\Documents\\\\GitHub\\\\kakaobootcamp\\\\personal mission\\\\NLP pipeline\\\\data\\\\SMSSpamCollection.csv'\n",
    "df = pd.read_csv(data_path, sep='\\s+', engine='python', names=['label', 'text'], error_bad_lines=False, warn_bad_lines=True)\n",
    "\n",
    "# 데이터 탐색\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "\n",
    "# 텍스트 전처리\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # 소문자 변환\n",
    "        text = text.lower()\n",
    "        \n",
    "        # 특수문자 제거\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "        \n",
    "        # 토큰화\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        \n",
    "        # 불용어 제거\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        \n",
    "        # 어간 추출\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "        \n",
    "        # 토큰을 다시 텍스트로 결합\n",
    "        preprocessed_text = ' '.join(tokens)\n",
    "        \n",
    "        return preprocessed_text\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# 데이터프레임에 전처리 적용\n",
    "df['text'] = df['text'].fillna('')\n",
    "df['preprocessed_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# 전처리된 데이터 확인\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f83f93b8-a957-443e-9f41-696a4c49da18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                    label  \\\n",
      "\"ham Go   until  jurong  point, crazy.. Available only in    bugis   n     great   world la      e    buffet... Cine     there got  amore   \n",
      "     Ok   lar... Joking  wif    u       oni...\"   NaN  NaN   NaN     NaN   NaN     NaN   NaN     NaN  NaN       NaN      NaN   NaN   None   \n",
      "     U    dun    say     so     early   hor...    U    c     already then  say...\" NaN   NaN     NaN  NaN       NaN      NaN   NaN   None   \n",
      "     Nah  I      don't   think  he      goes      to   usf,  he      lives around  here  though\" NaN  NaN       NaN      NaN   NaN   None   \n",
      "     Even my     brother is     not     like      to   speak with    me.   They    treat me      like aids      patent.\" NaN   NaN   None   \n",
      "\n",
      "                                                                                                                                       text  \n",
      "\"ham Go   until  jurong  point, crazy.. Available only in    bugis   n     great   world la      e    buffet... Cine     there got  wat...\"  \n",
      "     Ok   lar... Joking  wif    u       oni...\"   NaN  NaN   NaN     NaN   NaN     NaN   NaN     NaN  NaN       NaN      NaN   NaN     None  \n",
      "     U    dun    say     so     early   hor...    U    c     already then  say...\" NaN   NaN     NaN  NaN       NaN      NaN   NaN     None  \n",
      "     Nah  I      don't   think  he      goes      to   usf,  he      lives around  here  though\" NaN  NaN       NaN      NaN   NaN     None  \n",
      "     Even my     brother is     not     like      to   speak with    me.   They    treat me      like aids      patent.\" NaN   NaN     None  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "MultiIndex: 3927 entries, ('\"ham', 'Go', 'until', 'jurong', 'point,', 'crazy..', 'Available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet...', 'Cine', 'there', 'got') to ('\"ham', 'Rofl.', 'Its', 'true', 'to', 'its', 'name\"', nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan)\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   label   224 non-null    object\n",
      " 1   text    113 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 827.9+ KB\n",
      "None\n",
      "                                                                                                                                    label  \\\n",
      "\"ham Go   until  jurong  point, crazy.. Available only in    bugis   n     great   world la      e    buffet... Cine     there got  amore   \n",
      "     Ok   lar... Joking  wif    u       oni...\"   NaN  NaN   NaN     NaN   NaN     NaN   NaN     NaN  NaN       NaN      NaN   NaN   None   \n",
      "     U    dun    say     so     early   hor...    U    c     already then  say...\" NaN   NaN     NaN  NaN       NaN      NaN   NaN   None   \n",
      "     Nah  I      don't   think  he      goes      to   usf,  he      lives around  here  though\" NaN  NaN       NaN      NaN   NaN   None   \n",
      "     Even my     brother is     not     like      to   speak with    me.   They    treat me      like aids      patent.\" NaN   NaN   None   \n",
      "\n",
      "                                                                                                                                       text  \\\n",
      "\"ham Go   until  jurong  point, crazy.. Available only in    bugis   n     great   world la      e    buffet... Cine     there got  wat...\"   \n",
      "     Ok   lar... Joking  wif    u       oni...\"   NaN  NaN   NaN     NaN   NaN     NaN   NaN     NaN  NaN       NaN      NaN   NaN            \n",
      "     U    dun    say     so     early   hor...    U    c     already then  say...\" NaN   NaN     NaN  NaN       NaN      NaN   NaN            \n",
      "     Nah  I      don't   think  he      goes      to   usf,  he      lives around  here  though\" NaN  NaN       NaN      NaN   NaN            \n",
      "     Even my     brother is     not     like      to   speak with    me.   They    treat me      like aids      patent.\" NaN   NaN            \n",
      "\n",
      "                                                                                                                                   preprocessed_text  \n",
      "\"ham Go   until  jurong  point, crazy.. Available only in    bugis   n     great   world la      e    buffet... Cine     there got               wat  \n",
      "     Ok   lar... Joking  wif    u       oni...\"   NaN  NaN   NaN     NaN   NaN     NaN   NaN     NaN  NaN       NaN      NaN   NaN                    \n",
      "     U    dun    say     so     early   hor...    U    c     already then  say...\" NaN   NaN     NaN  NaN       NaN      NaN   NaN                    \n",
      "     Nah  I      don't   think  he      goes      to   usf,  he      lives around  here  though\" NaN  NaN       NaN      NaN   NaN                    \n",
      "     Even my     brother is     not     like      to   speak with    me.   They    treat me      like aids      patent.\" NaN   NaN                    \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# 데이터 로드\n",
    "data_path = 'C:\\\\Users\\\\dev\\\\Documents\\\\GitHub\\\\kakaobootcamp\\\\personal mission\\\\NLP pipeline\\\\data\\\\SMSSpamCollection.csv'\n",
    "df = pd.read_csv(data_path, sep='\\s+', engine='python', names=['label', 'text'], on_bad_lines='skip')\n",
    "\n",
    "# 데이터 탐색\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "\n",
    "# 텍스트 전처리\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # 소문자 변환\n",
    "        text = text.lower()\n",
    "        \n",
    "        # 특수문자 제거\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "        \n",
    "        # 토큰화\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        \n",
    "        # 불용어 제거\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        \n",
    "        # 어간 추출\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "        \n",
    "        # 토큰을 다시 텍스트로 결합\n",
    "        preprocessed_text = ' '.join(tokens)\n",
    "        \n",
    "        return preprocessed_text\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# 데이터프레임에 전처리 적용\n",
    "df['text'] = df['text'].fillna('')\n",
    "df['preprocessed_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# 전처리된 데이터 확인\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e93f50df-b2bf-4f0f-9322-68d0d0cf7230",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ee59834f-2daf-4bfc-84b4-3c898f6bc353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.7.5-cp311-cp311-win_amd64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.10-cp311-cp311-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.8-cp311-cp311-win_amd64.whl.metadata (8.6 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.3.0,>=8.2.2 (from spacy)\n",
      "  Downloading thinc-8.2.5-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.4.8-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Downloading typer-0.12.3-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\dev\\anaconda3\\lib\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\dev\\anaconda3\\lib\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\dev\\anaconda3\\lib\\site-packages (from spacy) (1.10.12)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dev\\anaconda3\\lib\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dev\\anaconda3\\lib\\site-packages (from spacy) (68.0.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\dev\\anaconda3\\lib\\site-packages (from spacy) (23.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.4.0-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\dev\\anaconda3\\lib\\site-packages (from spacy) (1.24.3)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\dev\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dev\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dev\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dev\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dev\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.6.2)\n",
      "Collecting blis<0.8.0,>=0.7.8 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading blis-0.7.11-cp311-cp311-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.3.0,>=8.2.2->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\dev\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\dev\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\dev\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.3.5)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.18.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\dev\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dev\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Collecting marisa-trie>=0.7.7 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.0-cp311-cp311-win_amd64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\dev\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\dev\\anaconda3\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\dev\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.0)\n",
      "Downloading spacy-3.7.5-cp311-cp311-win_amd64.whl (12.1 MB)\n",
      "   ---------------------------------------- 0.0/12.1 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.1 MB 8.3 MB/s eta 0:00:02\n",
      "   -- ------------------------------------- 0.7/12.1 MB 8.3 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 1.0/12.1 MB 8.1 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 1.4/12.1 MB 7.1 MB/s eta 0:00:02\n",
      "   ------ --------------------------------- 1.9/12.1 MB 7.9 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.3/12.1 MB 8.5 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.4/12.1 MB 7.5 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 2.5/12.1 MB 6.9 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 2.7/12.1 MB 6.5 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 3.1/12.1 MB 6.7 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 3.3/12.1 MB 6.5 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 3.4/12.1 MB 6.1 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 3.7/12.1 MB 6.1 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 4.2/12.1 MB 6.3 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 4.7/12.1 MB 6.6 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 5.2/12.1 MB 6.9 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.6/12.1 MB 7.0 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.7/12.1 MB 6.8 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 6.1/12.1 MB 6.8 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 6.6/12.1 MB 7.0 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 7.2/12.1 MB 7.1 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.7/12.1 MB 7.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 8.3/12.1 MB 7.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.8/12.1 MB 7.8 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 9.4/12.1 MB 7.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 10.0/12.1 MB 8.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.5/12.1 MB 8.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.0/12.1 MB 8.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 11.6/12.1 MB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.1 MB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.1 MB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.1 MB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.1 MB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.1 MB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.1 MB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.1 MB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.1/12.1 MB 8.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.1/12.1 MB 6.7 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.8-cp311-cp311-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.4.0-py3-none-any.whl (182 kB)\n",
      "   ---------------------------------------- 0.0/182.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 182.0/182.0 kB 5.4 MB/s eta 0:00:00\n",
      "Downloading murmurhash-1.0.10-cp311-cp311-win_amd64.whl (25 kB)\n",
      "Downloading preshed-3.0.9-cp311-cp311-win_amd64.whl (122 kB)\n",
      "   ---------------------------------------- 0.0/122.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 122.3/122.3 kB 3.6 MB/s eta 0:00:00\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.4.8-cp311-cp311-win_amd64.whl (479 kB)\n",
      "   ---------------------------------------- 0.0/479.7 kB ? eta -:--:--\n",
      "   ---------------------------------------  471.0/479.7 kB 9.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 479.7/479.7 kB 7.6 MB/s eta 0:00:00\n",
      "Downloading thinc-8.2.5-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.5 MB 15.5 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.1/1.5 MB 13.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.5/1.5 MB 11.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 10.5 MB/s eta 0:00:00\n",
      "Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
      "   ---------------------------------------- 0.0/47.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 47.2/47.2 kB 2.3 MB/s eta 0:00:00\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.3/50.3 kB 2.7 MB/s eta 0:00:00\n",
      "Downloading blis-0.7.11-cp311-cp311-win_amd64.whl (6.6 MB)\n",
      "   ---------------------------------------- 0.0/6.6 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/6.6 MB 16.5 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 1.1/6.6 MB 13.7 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 1.6/6.6 MB 12.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 2.2/6.6 MB 11.8 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 2.8/6.6 MB 11.9 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 3.3/6.6 MB 11.8 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 3.9/6.6 MB 11.9 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 4.4/6.6 MB 11.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.9/6.6 MB 11.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 5.5/6.6 MB 11.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 5.8/6.6 MB 11.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.3/6.6 MB 11.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.6/6.6 MB 11.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.6/6.6 MB 10.3 MB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.18.1-py3-none-any.whl (47 kB)\n",
      "   ---------------------------------------- 0.0/47.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 47.3/47.3 kB 2.3 MB/s eta 0:00:00\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 0.6/5.4 MB 12.0 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 1.1/5.4 MB 12.0 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 1.7/5.4 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 2.3/5.4 MB 12.0 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 2.8/5.4 MB 12.0 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 3.4/5.4 MB 12.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 4.0/5.4 MB 12.0 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 4.5/5.4 MB 11.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 5.0/5.4 MB 11.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  5.4/5.4 MB 11.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 10.8 MB/s eta 0:00:00\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading marisa_trie-1.2.0-cp311-cp311-win_amd64.whl (152 kB)\n",
      "   ---------------------------------------- 0.0/152.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 152.6/152.6 kB 4.4 MB/s eta 0:00:00\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, shellingham, murmurhash, marisa-trie, cloudpathlib, catalogue, blis, srsly, preshed, language-data, typer, langcodes, confection, weasel, thinc, spacy\n",
      "Successfully installed blis-0.7.11 catalogue-2.0.10 cloudpathlib-0.18.1 confection-0.1.5 cymem-2.0.8 langcodes-3.4.0 language-data-1.2.0 marisa-trie-1.2.0 murmurhash-1.0.10 preshed-3.0.9 shellingham-1.5.4 spacy-3.7.5 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.2.5 typer-0.12.3 wasabi-1.1.3 weasel-0.4.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "32a82278-a050-4603-9410-f9d1c2a03e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45e04750-009a-404f-9faf-3403db16a23f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dev\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dev\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               label  text\n",
      "0  ham\\tGo until jurong point, crazy.. Available ...   NaN\n",
      "1                 ham\\tOk lar... Joking wif u oni...   NaN\n",
      "2  spam\\tFree entry in 2 a wkly comp to win FA Cu...   NaN\n",
      "3  ham\\tU dun say so early hor... U c already the...   NaN\n",
      "4  ham\\tNah I don't think he goes to usf, he live...   NaN\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5574 entries, 0 to 5573\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   label   5574 non-null   object \n",
      " 1   text    0 non-null      float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 87.2+ KB\n",
      "None\n",
      "                                               label text preprocessed_text\n",
      "0  ham\\tGo until jurong point, crazy.. Available ...                       \n",
      "1                 ham\\tOk lar... Joking wif u oni...                       \n",
      "2  spam\\tFree entry in 2 a wkly comp to win FA Cu...                       \n",
      "3  ham\\tU dun say so early hor... U c already the...                       \n",
      "4  ham\\tNah I don't think he goes to usf, he live...                       \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# 필요한 NLTK 데이터 다운로드\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# 데이터 로드\n",
    "data_path = 'C:\\\\Users\\\\dev\\\\Documents\\\\GitHub\\\\kakaobootcamp\\\\personal mission\\\\NLP pipeline\\\\data\\\\SMSSpamCollection.csv'\n",
    "try:\n",
    "    df = pd.read_csv(data_path, sep='\\t', names=['label', 'text'], encoding='utf-8')\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "\n",
    "# 데이터 탐색\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "\n",
    "# 텍스트 전처리\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # 소문자 변환\n",
    "        text = text.lower()\n",
    "        \n",
    "        # 특수문자 제거\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "        \n",
    "        # 토큰화\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        \n",
    "        # 불용어 제거\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        \n",
    "        # 어간 추출\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "        \n",
    "        # 토큰을 다시 텍스트로 결합\n",
    "        preprocessed_text = ' '.join(tokens)\n",
    "        \n",
    "        return preprocessed_text\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# 데이터프레임에 전처리 적용\n",
    "df['text'] = df['text'].fillna('')\n",
    "df['preprocessed_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# 전처리된 데이터 확인\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ff11522-3aa8-4dfa-a7bc-365c9aa06aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               label  text\n",
      "0  ham\\tGo until jurong point, crazy.. Available ...   NaN\n",
      "1                 ham\\tOk lar... Joking wif u oni...   NaN\n",
      "2  spam\\tFree entry in 2 a wkly comp to win FA Cu...   NaN\n",
      "3  ham\\tU dun say so early hor... U c already the...   NaN\n",
      "4  ham\\tNah I don't think he goes to usf, he live...   NaN\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5574 entries, 0 to 5573\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   label   5574 non-null   object \n",
      " 1   text    0 non-null      float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 87.2+ KB\n",
      "None\n",
      "                                               label text preprocessed_text\n",
      "0  ham\\tGo until jurong point, crazy.. Available ...                       \n",
      "1                 ham\\tOk lar... Joking wif u oni...                       \n",
      "2  spam\\tFree entry in 2 a wkly comp to win FA Cu...                       \n",
      "3  ham\\tU dun say so early hor... U c already the...                       \n",
      "4  ham\\tNah I don't think he goes to usf, he live...                       \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "# 데이터 로드\n",
    "data_path = 'C:\\\\Users\\\\dev\\\\Documents\\\\GitHub\\\\kakaobootcamp\\\\personal mission\\\\NLP pipeline\\\\data\\\\SMSSpamCollection.csv'\n",
    "try:\n",
    "    df = pd.read_csv(data_path, sep='\\t', names=['label', 'text'], encoding='utf-8')\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "\n",
    "# 데이터 탐색\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "\n",
    "# 텍스트 전처리\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # 소문자 변환\n",
    "        text = text.lower()\n",
    "        \n",
    "        # 특수문자 제거\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "        \n",
    "        # 토큰화\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        \n",
    "        # 불용어 제거\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        \n",
    "        # 어간 추출\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "        \n",
    "        # 토큰을 다시 텍스트로 결합\n",
    "        preprocessed_text = ' '.join(tokens)\n",
    "        \n",
    "        return preprocessed_text\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# 데이터프레임에 전처리 적용\n",
    "df['text'] = df['text'].fillna('')\n",
    "df['preprocessed_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# 전처리된 데이터 확인\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "629cd8b9-f2a2-4509-be92-55e0d8bd0210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               label  text\n",
      "0  ham\\tGo until jurong point, crazy.. Available ...   NaN\n",
      "1                 ham\\tOk lar... Joking wif u oni...   NaN\n",
      "2  spam\\tFree entry in 2 a wkly comp to win FA Cu...   NaN\n",
      "3  ham\\tU dun say so early hor... U c already the...   NaN\n",
      "4  ham\\tNah I don't think he goes to usf, he live...   NaN\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5574 entries, 0 to 5573\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   label   5574 non-null   object \n",
      " 1   text    0 non-null      float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 87.2+ KB\n",
      "None\n",
      "                                               label text preprocessed_text\n",
      "0  ham\\tGo until jurong point, crazy.. Available ...                       \n",
      "1                 ham\\tOk lar... Joking wif u oni...                       \n",
      "2  spam\\tFree entry in 2 a wkly comp to win FA Cu...                       \n",
      "3  ham\\tU dun say so early hor... U c already the...                       \n",
      "4  ham\\tNah I don't think he goes to usf, he live...                       \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "# 데이터 로드\n",
    "data_path = 'C:\\\\Users\\\\dev\\\\Documents\\\\GitHub\\\\kakaobootcamp\\\\personal mission\\\\NLP pipeline\\\\data\\\\SMSSpamCollection.csv'\n",
    "try:\n",
    "    df = pd.read_csv(data_path, sep='\\t', names=['label', 'text'], engine='python', encoding='utf-8')\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "\n",
    "# 데이터 탐색\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "\n",
    "# 텍스트 전처리\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # 소문자 변환\n",
    "        text = text.lower()\n",
    "        \n",
    "        # 특수문자 제거\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "        \n",
    "        # 토큰화\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        \n",
    "        # 불용어 제거\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        \n",
    "        # 어간 추출\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "        \n",
    "        # 토큰을 다시 텍스트로 결합\n",
    "        preprocessed_text = ' '.join(tokens)\n",
    "        \n",
    "        return preprocessed_text\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# 데이터프레임에 전처리 적용\n",
    "df['text'] = df['text'].fillna('')\n",
    "df['preprocessed_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# 전처리된 데이터 확인\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45d5632a-d545-4897-874d-7125ee591ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading data: Specified \\n as separator or delimiter. This forces the python engine which does not accept a line terminator. Hence it is not allowed to use the line terminator as separator.\n",
      "                                               label text preprocessed_text\n",
      "0  ham\\tGo until jurong point, crazy.. Available ...                       \n",
      "1                 ham\\tOk lar... Joking wif u oni...                       \n",
      "2  spam\\tFree entry in 2 a wkly comp to win FA Cu...                       \n",
      "3  ham\\tU dun say so early hor... U c already the...                       \n",
      "4  ham\\tNah I don't think he goes to usf, he live...                       \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5574 entries, 0 to 5573\n",
      "Data columns (total 3 columns):\n",
      " #   Column             Non-Null Count  Dtype \n",
      "---  ------             --------------  ----- \n",
      " 0   label              5574 non-null   object\n",
      " 1   text               5574 non-null   object\n",
      " 2   preprocessed_text  5574 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 130.8+ KB\n",
      "None\n",
      "                                               label text preprocessed_text\n",
      "0  ham\\tGo until jurong point, crazy.. Available ...                       \n",
      "1                 ham\\tOk lar... Joking wif u oni...                       \n",
      "2  spam\\tFree entry in 2 a wkly comp to win FA Cu...                       \n",
      "3  ham\\tU dun say so early hor... U c already the...                       \n",
      "4  ham\\tNah I don't think he goes to usf, he live...                       \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# 데이터 로드\n",
    "data_path = 'C:\\\\Users\\\\dev\\\\Documents\\\\GitHub\\\\kakaobootcamp\\\\personal mission\\\\NLP pipeline\\\\data\\\\SMSSpamCollection.csv'\n",
    "try:\n",
    "    df = pd.read_csv(data_path, sep='\\n', names=['raw'], encoding='utf-8')\n",
    "    df[['label', 'text']] = df['raw'].str.split('\\t', 1, expand=True)\n",
    "    df.drop(columns=['raw'], inplace=True)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "\n",
    "# 데이터 탐색\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "\n",
    "# 텍스트 전처리\n",
    "def preprocess_text(text):\n",
    "    if isinstance(text, str):\n",
    "        # 소문자 변환\n",
    "        text = text.lower()\n",
    "        \n",
    "        # 특수문자 제거\n",
    "        text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "        \n",
    "        # 토큰화\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        \n",
    "        # 불용어 제거\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        tokens = [token for token in tokens if token not in stop_words]\n",
    "        \n",
    "        # 어간 추출\n",
    "        stemmer = PorterStemmer()\n",
    "        tokens = [stemmer.stem(token) for token in tokens]\n",
    "        \n",
    "        # 토큰을 다시 텍스트로 결합\n",
    "        preprocessed_text = ' '.join(tokens)\n",
    "        \n",
    "        return preprocessed_text\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# 데이터프레임에 전처리 적용\n",
    "df['text'] = df['text'].fillna('')\n",
    "df['preprocessed_text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# 전처리된 데이터 확인\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0266f803-1897-465e-88b1-afe08725ccce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
